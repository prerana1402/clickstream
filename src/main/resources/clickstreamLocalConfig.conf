app {
  //name = "ClickstreamDataPipeline"
  version = "1.0"


  input {
    clickstreamPath = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_in/clickstream_log(1).csv"
    itemsetPath = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_in/item_data(1).csv"
  }

  output {
    path = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/click_stream_event_item"
    nullClickstream = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/nullClickstream"
    nullItemset = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/nullItemset"
    duplicateClickstream = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/duplicateClickstream"
    duplicateItemset = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/duplicateItemset"
    invalidItemPrice = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/invalidItemPrice"
    invalidEventTimestamp = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/invalidEventTimestamp"
  }

  spark {
    master = "local[*]"
    appName = "ClickstreamDataPipeline"
    logLevel = "ERROR"
    //spark.executor.memory="2g"
    //spark.default.parallelism=4
    //spark.sql.shuffle.partitions=10
    //spark.streaming.backpressure.enabled=true
    //spark.streaming.kafka.maxRatePerPartition=1000
  }

  jdbc {
    jdbcUrl = "jdbc:mysql://localhost:3306/clickstream"
    jdbcUser = "root"
    jdbcPassword = "root1234"
  }
  // config validation
  // stage.conf & prod.conf
}