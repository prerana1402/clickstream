app{
  name="ClickstreamDataPipeline"
  version="1.0"
}

input{
  path1="/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_in/clickstream_log(1).csv"
  path2="/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_in/item_data(1).csv"
}

output{
  path="/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/click_stream_event_item"
  nullClickstream = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/nullClickstream"
  nullItemset = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/nullItemset"
  duplicateClickstream = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/duplicateClickstream"
  duplicateItemset = "/Users/preranag/IdeaProjects/demo.scala/src/test/scala/data_out/duplicateItemset"
}

spark{
  master="local[*]"
  appName=${app.name}
  logLevel="ERROR"
  //spark.executor.memory="2g"
  //spark.default.parallelism=4
  //spark.sql.shuffle.partitions=10
  //spark.streaming.backpressure.enabled=true
  //spark.streaming.kafka.maxRatePerPartition=1000
}

jdbc{
  jdbcUrl = "jdbc:mysql://localhost:3306/clickstream"
  jdbcUser = "root"
  jdbcPassword = "root1234"
}
// config validation
// stage.conf & prod.conf